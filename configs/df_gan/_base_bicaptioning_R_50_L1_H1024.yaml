# -----------------------------------------------------------------------------
# Base config: VirTex pretraining for our "base" bicaptioning model:
# ResNet-50 + (L = 1, H = 1024) transformer trained for 500K iterations.
# -----------------------------------------------------------------------------
RANDOM_SEED: 100
AMP: false 
CUDNN_BENCHMARK: true
CUDNN_DETERMINISTIC: false

DATA:
  ROOT: "datasets/coco"
  TOKENIZER_MODEL: "datasets/vocab/coco17_10k.model"
  VOCAB_SIZE: 10000
  UNK_INDEX: 0
  SOS_INDEX: 1
  EOS_INDEX: 2
  MASK_INDEX: 3

  IMAGE_CROP_SIZE: 128
  MAX_CAPTION_LENGTH: 30

  IMAGE_TRANSFORM_TRAIN:
    - "random_resized_crop"
    - "horizontal_flip"
    #- "color_jitter"
    - "normalize"

  IMAGE_TRANSFORM_VAL:
    - "smallest_resize"
    - "center_crop"
    - "normalize"

TEXT_ENCODER:
  NAME: "capD" # "damsm" # "random"
  DIR: '' # "datasets/DAMSMencoders/coco/text_encoder100.pth"
  EMBEDDING_SIZE: 2048
  FROZEN: true

GENERATOR:
  NAME: "df"
  NOISE_SIZE: 100
  FEATURE_SIZE: 32

DISCRIMINATOR:
  NAME: "capD"
  VISUAL:
    NAME: "torchvision::resnet50" # "df"
    FEATURE_SIZE: 2048
    PRETRAINED: false 
    FROZEN: false
  TEXTUAL:
    NAME: "transdec_postnorm::L1_H1024_A16_F4096"
    DROPOUT: 0.1
    DECODER:
      NAME: "beam_search"
      BEAM_SIZE: 5
  LOGITOR:
    NAME: "df" #"proj"
    H: 1024

TRAIN:
  BATCH_SIZE: 24
  NUM_ITERATIONS: 500000

GAN_LOSS:
  TYPE: "hinge"
  D_LOSS_COMPONENT: "logit,magp" #cap, 
  G_LOSS_COMPONENT: "logit" #cap, fa, fa_const
  LOGIT_INPUT: "visual_features"
  LOGIT_STOP_GRAD: False
  FA_FEATURE: "visual_features"

OPTIM:
  G:
    OPTIMIZER_NAME: "adam"
    VISUAL_LR: 0.0001
    BETAS: [0.0, 0.9]
    CLIP_GRAD_NORM: 0.
    UPDATE_EMB: false

  D:
    OPTIMIZER_NAME: "adam"
    VISUAL_LR: 0.0004
    TEXT_LR: 0.0004
    BETAS: [0.0, 0.9]
    CLIP_GRAD_NORM: 0.
    UPDATE_EMB: false

